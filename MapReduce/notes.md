# MapReduce

## 背景

MapReduce 的诞生背景是 Google 内部有大量的爬虫文档、web 请求日志等大量数据计算的场景，最初通过代码来计算处理数据的方式让这块代码变得越来越复杂和不可维护，也因此，Google 尝试开发一个 framework，把这些细节抽象，对用户隐藏，而用户只需要编写 Map、Reduce 函数，然后使用 MapReduce 的 library 执行计算即可。

## 执行

另一方面，通过定义的分区方法（用户可以通过参数定义，比如 **hash(key) mod R** ），Map 输出的数据会根据 Key 分区成 R 份（这个R同样也可以通过参数定义）。

具体执行步骤如下：

1. MapReduce 框架会负责从输入文件（一般是保存在GFS上）读取数据然后产出 k/v 对，并把它们分区，一般切分成M份，每份 16MB ~ 64MB（用户可以通过参数定义），然后克隆出多份副本，分别分发给集群里多台机器；

2. 其中一份副本会分发给 master，其他即是负责该任务的 worker。因为分别有 M 份和 R 份数据，所以自然也就有这么些任务数，master 会负责分配空闲 worker 对应的任务；

3. 分配了 map 任务的 worker，会从相应的 input split 读取数据。它会把输入数据解析成 k/v 对，然后遍历每对数据，执行用户定义的 Map 方法。最终把执行了 Map 方法后的中间 k/v 数据存到内存；

4. 缓存的中间数据会被定期地写入到本地磁盘，并且根据分区函数（上文提到的 R 里面的分区方法）分到对应的 R 区域。这些存储到本地磁盘的地址会被传回给 master，然后最终会被 master 发给对应的 reduce worker；

5. 当 reduce worker 收到 master 发来的这些分区好的 R 分片数据的位置信息时，它会使用 RPC 从 map worker 那里读取缓存了的数据，**在读完所有中间数据后**，reduce worker 会将数据排序，这样相同的 key 会被分到一起。因为一般同一个 reduce worker 会收到相当多不同 key 的数据，因此排序是必要的，如果中间数据放内存里很大的话，可能还需要执行 **外部排序**；

6. reduce worker 会遍历排序好的中间数据，然后针对每一个遇到的唯一的中间 key ，它会将该 key 及对应的数据集传给用户的 Reduce 函数。Reduce 函数的输出会被追加到该 reduce 分区下的一个最终输出里；

7. 当所有 map 和 reduce 任务完成后，master 会唤醒用户程序。MapReduce 的调用会 return 给用户程序。

## master 做的事情

master 会维护每个 map\reduce task 的状态（idle、in-progress、completed），以及运行任务的每个worker节点的身份。

对于每个完成的 map 任务，master 会保存 map 任务产出的 R 中间文件的区域位置及大小等信息。完成时会通知正在处理 reduce 任务的 worker。

## 错误容忍

### 1、Worker 故障

master 会定期 ping 每个 worker 节点。如果在一个确定的时间里没有响应的话，master 会把该 worker 标记为失败。任何已经完成了的 map 任务会被重置为 idle（因为结果保存在那台故障节点的本地磁盘上，如果是保存到了 GFS 的话就不用重新分配重新跑了...），随后会被重新调度给其他 worker 节点，类似地，进行中的 map、reduce 任务也会被重置，重新分配给其他 worker 节点。

即便有大规模 worker 节点故障，MapReduce 会重新执行那些故障节点上的任务，然后尝试不断取得进展，直到最终完成任务。

### 2、Master 故障

master 端存储的状态数据（如当前分配的任务信息等）会定期写入 checkpoint 。随后 master 的新副本可以从 checkpoint 恢复数据启动。如果 master 挂了，客户端可以检查这个情况然后后面重试。

### 3、存在故障的一些语境

1、MapReduce 依赖于 map\reduce 任务输出的原子性提交来实现一致性；

2、每个处理中的任务会维护输出到一个私有的临时文件，reduce 会产出一份，map 会产出 R 份这样的文件（每个对应一个 reduce 任务），如果有 map 任务完成了，那个完成 map 任务的 worker 会发消息给 master，然后带上 R 份临时文件的信息，如果第一次收到它会记录这 R 份文件信息，如果 master 已经收到过它会忽略该信息；当一个 reduce 任务完成任务时，reduce worker 会原子性地将该临时输出文件重命名为最终的输出文件，如果执行过多次，那可能会有多份最终输出，MapReduce **通过底层文件系统提供的原子性重命名操作**来实现只有一份产出数据；

3、如果用户执行任务一个M可能产出的R每次执行不一样的话，不同的 map 执行可能会被不同的 R 任务执行产出不同的结果。

## 地区性

MapReduce 的核心瓶颈在网络带宽（ MIT 6.824 的讲师说 2020 年的情况大有改善，网络环境不再是之前核心交换这样的有带宽瓶颈的架构了）。GFS 把每个文件切成 64MB 的块，然后每块都会保存多份副本（一般是3）到不同的机器上。master 会把输入文件的地理位置信息考虑在内，尽量把 map 任务调度到有该输入数据副本的机器上。失败了的话，会尝试调度到离输入数据近的地方（比如同一个网络交换机下）。在一个拥有众多 worker 节点的集群里执行大型 MapReduce 操作时，绝大多数的输入数据是从本地读取的，然后不消耗网络带宽。

## 任务粒度

每个 map/reduce 任务对大概会占用1个byte的内存，总的状态信息占用内存会是 O(M*R) ，需要做的调度决策是 O(M+R)。

每份输入数据大小大概是 16MB 到 64MB（这样地区方面的优化也能做到最高效），并且分配的 R 会是机器数量的小几倍的量级。常常采取的配置是，M = 200,000，R = 5,000，worker 机器数量则是 2,000。

## 备份任务

一个常见的影响 MapReduce 操作耗时的问题就是 “straggler”：在计算过程中花费异常的时间来完成 map/reduce 任务的 worker 节点。糟糕的磁盘写入，或者被调度了其他类型任务，又或者是因为机器初始化的 bug 导致进程缓存被禁用因而计算速度下降一百倍，这都有可能导致这些机器变成所谓的 “straggler”。MapReduce 提出了一个通用的机制来缓解这类问题，在一个 MapReduce 操作接近完成时，master 会调度一些剩余正在执行任务的一些备份性质的执行，无论主任务还是兜底任务完成，该任务都会被标记为完成，这带来了巨大的性能提升（例子说是如果禁用会拉长44%的时间）。

## 细化功能：分区函数

默认的功能能解决绝大多数场景下的需求，但是往往仍然存在一些自定义的用户需求，比如分区，默认的分区函数式通过哈希（比如 hash(key) mod R ）来得到目标的 R 集合，这样得出的结果是最均衡的，用户也可以自定义分区函数，比如在处理URL时，可能需要所有同一host的数据放到同一份输出文件里，这样可以用 hash(Hostname(urlkey)) mod R 这样的分区函数来实现。

## 细分功能：排序保证

MapReduce 会保证在一个给定的分区里，中间生成的kv对满足key是升序的。

## 细分功能：组合函数

有时候比如单词计数这种，会有大量重复和关联性质的操作，这时候就可以在map和reduce中间增加定义一个组合函数，而这个组合函数一般是在写入到map产出结果文件之前执行的，附录有提供一个具体的例子，它举了一个通过局部sum来节省网络带宽的这样的组合函数的运用。

## 细分功能：输入输出类型

MapReduce 支持读取多种格式的输入数据。比如 “文本” 模式的输入就是把文件里的每一行作为一个kv对，key是文件里的offset，值则是该行的内容。另外一种常见的支持格式是存放一个根据key排序了的kv对序列。每种输入类型的实现需要知道怎么把自己切分成一个有意义的range作为单独的map任务处理的数据（比如文本模式的range切分需要确保只在行边界时split）。用户也可以通过自定义一个简单的reader接口来定义新的输入类型，但是绝大多数还是使用少量预定义的输入类型。

reader的数据源不局限于文件，也可以是从数据库、内存等地方读取。同样地，MapReduce也支持自定义新的输出类型。

## 细分功能：副作用

有时候用户可能会需要在执行 map 以及/或者 reduce 操作时生成一些额外的辅助文件。这种副作用的原子性和幂等性是依赖应用writer自行实现的，一般来说是通过写一个临时文件然后在完全生成数据后原子性地重命名该文件来实现。

值得一提的是，多份输出文件的两阶段提交的原子性是无法保证的，一致性需求要靠应用自己保证，但是实践经验来看，这个往往不是什么大问题。

## 细分功能：跳过坏数据

MapReduce 提供一个可选参数，允许用户指定是否在确定某些坏数据导致崩溃时跳过这些坏数据。这个功能是通过在每个worker进程上安装一个signal handler，然后它会捕获segmentation violation和总线错误，在调用用户执行map或者reduce操作前，它会将这次执行的序列号参数保存到一个全局变量里，然后如果用户执行代码发送signal的话，signal handler会帮忙发送一个最后的UDP包（里面包含了序列号数据）给到 MapReduce 的master。当 master 检测到不止一次的故障时，它会标记该条数据应当跳过，然后在下次执行时会通知对应的Map/Reduce任务跳过这样的记录。

## 细分功能：本地执行

为了让调试 Map、Reduce 函数更方便，官方还开发了一个在本地机器上顺序执行的 MapReduce 版本，并且为用户提供了相关的控件，可以将计算任务限制在某些特定的 map 任务。用户可以通过带上一个特别的参数来调用他们的程序并且可以方便地调试和测试。

## 细分功能：状态信息

master 会运行一个内部的HTTP服务器然后对外暴露一系列的状态信息。用户可以查看当前完成了多少任务，有多少正在执行的任务，输入的字节数，中间数据的字节数，输出的字节数，及处理频率等。它也包含了每个任务输出的标准错误和标准输出文件的链接信息。用户可以评估执行信息是否合理，是需要加资源还是算的有点太慢了。

另外，第一级的状态信息会展示有多少失败的worker，然后失败的时候它们各自正在处理多少 map\reduce 任务。这个可以用来调试用户代码的问题。

## 细分功能：计数器

MapReduce 还额外提供了一个 Counter 的对象，它会自己默认维护一些 Counter 对象，比如当前处理过的kv对数量以及输出的kv对数量。每当用户使用Counter对象时，它需要创建一个命名了的Counter对象，然后再在map/reduce方法里对该对象执行递增操作，master会聚合每个成功执行的map/reduce任务上报的Counter数据，然后最终在MapReduce操作完成时把结果返回给用户代码。Counter数据会展示在状态页面，如果有重复执行的情况（比如失败重试和备份任务执行这些情况），map会自己判断，给相同的map/reduce任务上报的结果去重。

## 性能

分别针对一个模式匹配和排序的场景进行大规模的性能测试。

针对Grep的测试发现启动时会有一个overhead，它来自于将程序分发到所有worker机器，以及和GFS交互打开输入数据文件以及获取必需的地区性信息时的成本。在禁用backup任务的情况下会有一个长尾。在出现一定规模的机器故障的情况下，处理速率会很快下降，但是在worker恢复后处理速率又会恢复正常。

## 经验收获

MapReduce 是2003年2月发布的第一个版本，2003年的八月份引入了地区性的性能优化，任务执行的动态负载均衡等。他们发现在 Google 内部下面这些问题适用于通过 MapReduce 来解决：

* 大规模机器学习问题；
* Google News 和 Froogle 产品的集群化问题；
* 解压数据来产出常见查询的报告（比如Google Zeitgeist）；
* 为新的实验和产品解压网站网页的属性（比如从大量网页中提取地理位置用作本地搜索）；
* 大规模图形计算

大规模站点文档数据索引系统改用 MapReduce 形式之后有如下收获：

* index 系统本身的代码变得更简单易懂了；
* MapReduce性能很好，一些改造实施也变得更加轻松了；
* index 程序的可运维性更好了，要改善性能也只需要增加机器到indexing的MapReduce集群即可。

## 相关工作

MapReduce通过一个受限的编程模型得以将一些并行执行时的细节隐藏，而它背后仍然依赖一个集群管理系统来帮忙协同master和worker。它的地区性优化功能源自于active disk这样的技术的启发，备份任务机制源自于像Charlotte系统这样的eager scheduling机制，并且通过引入忽略坏记录解决了因为指定任务的失败造成的重复性故障。排序设施有点像Now-Sort，它会切成M份数据然后各自排序后再给到R，每个R再在本地自行处理排序。允许冗余执行以及地区性感知的调度能力和BAD-FS也有些相似之处。

## 结论

1、引入MapReduce后，为开发人员隐藏了诸多执行细节，大大减轻了他们的负担；

2、很多问题均可以通过MapReduce这样的模型范式来解决；

3、对于大规模计算问题的解决，MapReduce提出了一个统一的方案，并且节省了大量的之前可能重复浪费的机器资源；

4、限制编程模式有利于并行和分布式计算，并且利于实现计算的容错；

5、网络带宽是一个瓶颈性质的稀缺资源；

6、冗余执行可以用来减少较差性能的机器带来的影响，而且有利于处理机器故障及数据损失

## 注意

1. 数据类型是可以由用户定义的，并不限制一定要是string；

2. reduce 的结果一般也是按照分区产出到对应的 R 输出文件，用户一般是不需要自行合并结果的，他们可以再把它们作为下一个 MapReduce 过程的输入数据；

3. 一旦发生故障后的重新调度的话，其他执行 reduce 任务的 worker 会收到这个信息的通知，并且会转向从代替的节点读取数据；

## 疑问

Q1. 这么大的数据排序也会是一个问题，怎么解决的呢？

Q2. map-reduce 的模式，尽管 map 会分发到各台机器，是分片了的，但是最终相同的 key 仍然会需要交给 reduce worker 来计算处理，这块是怎么优化性能的呢？

Q3. Map 产出的数据保存到本地磁盘这个过程，如果有数据损坏怎么办？是否是保存到了 GFS ？如果是的话，是否用到了临时文件避免部分写的情况？

Q4. 有哪些常见的 R partition 方法？

Q5. Map 数据的分区一般是按照什么规则去做的？

Q6. Reduce worker 怎么知道什么时候 ready 去跑了？换句话说，怎么知道什么时候 map 完成了一个 R 分区所有数据的执行？这块会不会是一个潜在的故障点？
